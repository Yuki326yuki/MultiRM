seed: 42

model:
  pretrain: meta-llama/Meta-Llama-3-8B-Instruct
  tokenizer_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
  flash_attn2: true
  lora: false           # A800 80G 完全可以全参 finetune；若想省显存设 true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

# 各类型：类别数 m_k、温度 tau_k、α_k (CE 权重)、μ_k (MSE 权重)
types:
  overall:               { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  helpfulness:           { m: 3, tau: 0.8, alpha: 1.0, mu: 1.0 }
  correctness:           { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  coherence:             { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  complexity:            { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  verbosity:             { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  truthfulness:          { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  honesty:               { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }
  instruction_following: { m: 3, tau: 1.0, alpha: 1.0, mu: 1.0 }

data:
  train_path: data/multirm_train.jsonl

train:
  batch_size: 4          # A800 80G + bf16，8B 模型可尝试 4
  max_len: 2048
  steps: 20000
  lr: 3.0e-4
  weight_decay: 0.01
  grad_clip: 1.0
  log_every: 20
  save_every: 1000
  out_dir: outputs/multirm-8b
  beta_kl: 0.0          # 目前没有 ref_prob，先关掉；以后接参考模型再开
  l2_lambda: 0.0        # 如需显式 L2 正则，这里设 >0

